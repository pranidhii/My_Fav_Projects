Problem StatementThe data set comprises of Reddit's comments for different brands of cars. For a new user visiting the pages, the content is overwhelming at the first sight. If he has a priority list for certain features in the automobile, he might not be able to reach the most valuable comments given the number of comments.
SolutionTo create a better user experience for these users, we planned to create filters of the most important topics under play in each of the brands comments, so that the user can filter out the comments related to these important topics.
Related WorkAmazon has the feature of filters on its reviews. So, for example, if a user is looking for a certain quality in the product such as ‘look’ , he can just click on the ‘look’ or ‘appearance’ filter and the comments that talk about the look or appearance of the product will appear, providing the user more valuable comments to read. We are drawing example from this case and are trying to create the filters for the Reddit comments.
Approach & MethodologyThe first step would be to look for the keywords in the text corpus and use them to create topics. As there are various methods to do the same, determine the best approach to get the most relevant set of keywords. Hence, explore different methods to finalize which methodology works the best and produces the most relevant filters.The methodology involved was to first create a corpus of stop words that was relevant to the dataset. And then perform the following methods on the dataset :Topic Modeling with Latent Dirichlet AllocationWord2Vec – Create embeddings and use KMeans algorithm on the embeddings to create clustersBERT – Use BERT methodology to create topic modeling using the UMAP and DBSCAN to establish the number of topics and then extract the number of topics.
Topic ModelingWhat is a topic model?A Topic Model can be defined as an unsupervised technique to discover topics across various text documents. These topics are abstract in nature, i.e., words which are related to each other form a topic. Similarly, there can be multiple topics in an individual document. For the time being, let’s understand a topic model as a black box, as illustrated in the below figure:This black box (topic model) forms clusters of similar and related words which are called topics. These topics have a certain distribution in a document, and every topic is defined by the proportion of different words it contains.Topic modeling helps in exploring large amounts of text data, finding clusters of words, similarity between documents, and discovering abstract topics. To provide a solution to the problem we identified, we have tried a couple of approaches to implement Topic Modeling on the Reddit Data Comments

Topic Modeling with Transformers & BERTThis section will briefly explain what “transformers” is and how it helps with analyzing text data. In addition, I will illustrate the differences between BERT and other models such as Word2Vec.Transformers is a library that provides general-purpose architectures such as BERT, GPT-2, DistilBERT for Natural Language Understanding and Natural Language Generation.BERT is one of the models that are available in the transformer library. BERT stands for Bidirectional Encoder Representations from Transformers) and as its name suggests, it read the text sequence all at once from both directions instead of just one, which allows it to understand the meaning of a specific word based on its given context. An example would be, suppose we have two lines of text each contains the word ‘bank’: “Kids were playing alongside a riverbank” and “The man went into the bank to deposit a large check”. In word2Vec, the model will not differentiate the meaning of these two “banks”, while BERT can do so because it was trained with masks on certain words and it was designed to distinguish different meanings of the same words based on the context.BERT is powerful because it was pre-trained on BookCorpus, which covers text data from 11,038 unpublished books and the entire English Wikipedia. This creates billions of words and text for BERT to learn the contextual meanings of a specific word.For our project, we leveraged a package named ‘Sentence-Transformer’ and used BERT to perform sentence embedding. Our objective was to convert raw text data into vectors in high dimensions and once we have the vectors available, we can cluster them into different topics using UMAP. Because we are working with 6 distinct brands of cars, we are interested in examining the topic differences between these brands. Clustering text sequences will help us achieve our goal and give us a general idea of what people talk the most under each Subreddit.