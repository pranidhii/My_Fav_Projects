{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "This notebook solves the problem of classifying the job title into a broad category. The training data is a set of job titles scraped from various job categories such as sales, human resource, nurse, etc. This notebook uses Multinomial Naive Bayes algorithm to perform supervised multi class with single label text classification. \n",
    "\n",
    "The multinomial Naive Bayes classifier is suitable for classification with discrete features (e.g., word counts for text classification). In this document, two methods to create discrete features is used - Count Vectorizing the document ( counting the frequency of the words in each corpus) then running a tf-idf to get the weightage of the word in the whole corpus. The final model is pipelined with CV(), TF-IDF() and MultinomialNB()."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup\n",
    "\n",
    "The S3 bucket and prefix that you want to use for training and model data. This is currently written in US-EAST-2 region as the Notebook Instance, training, and hosting.\n",
    "\n",
    "The IAM role ARN is used to give SageMaker access to the data. It can be fetched using the get_execution_role method from sagemaker python SDK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import boto3\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.sklearn.processing import SKLearnProcessor\n",
    "\n",
    "\n",
    "region = boto3.session.Session().region_name\n",
    "\n",
    "role = sagemaker.get_execution_role()\n",
    "sklearn_processor = SKLearnProcessor(framework_version='0.20.0',\n",
    "                                     role=role,\n",
    "                                     instance_type='ml.m5.large',\n",
    "                                     instance_count=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The data is read from the S3 bucket and then first few rows are checked to confirm the right training data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Job Title</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>merchandiser</td>\n",
       "      <td>Merchandiser</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>fraud specialist train work home</td>\n",
       "      <td>Operations</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>family therapist</td>\n",
       "      <td>Therapist</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>human resource director</td>\n",
       "      <td>Human Resources</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>respiratory therapist</td>\n",
       "      <td>Therapist</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          Job Title            Label\n",
       "0                      merchandiser     Merchandiser\n",
       "1  fraud specialist train work home       Operations\n",
       "2                  family therapist        Therapist\n",
       "3           human resource director  Human Resources\n",
       "4             respiratory therapist        Therapist"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "bucket = 'My-Fav-Bucket'\n",
    "prefix = 'Text-Classification'\n",
    "\n",
    "input_data = 's3://{}/{}/{}'.format(bucket, prefix, 'train.csv')\n",
    "df = pd.read_csv(input_data, names=['Job Title','Label'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing\n",
    "We need to preprocess the training data to make it ready to be ingested by the training model. Primarily the nltk library is used to free it from alpha numeric, punctuation, etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting preprocessing.py\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "%%writefile preprocessing.py\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "import warnings \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from glob import glob\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "import json \n",
    "from pandas.io.json import json_normalize\n",
    "import os\n",
    "import sagemaker\n",
    "from sklearn.exceptions import DataConversionWarning\n",
    "warnings.filterwarnings(action='ignore', category=DataConversionWarning)\n",
    "\n",
    "bucket = 'altru-comprehend-input'\n",
    "prefix = 'Data_Sagemaker'\n",
    "\n",
    "data = 's3://{}/{}/{}'.format(bucket, prefix, 'train.csv')\n",
    "test_data = 's3://{}/{}/{}'.format(bucket, prefix, 'validation.csv')\n",
    "\n",
    "if __name__=='__main__':\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--train-test-split-ratio', type=float, default=0.2)\n",
    "    args, _ = parser.parse_known_args()\n",
    "    \n",
    "    input_data_path = os.path.join('/opt/ml/input/data', 'train.csv')\n",
    "    print('Reading input data from {}'.format(input_data_path))\n",
    "\n",
    "    df = pd.read_csv(data, names=['Job Title','Label'])\n",
    "    df.dropna(inplace=True)\n",
    "    df.drop_duplicates(inplace=True)\n",
    "    df_test = pd.read_csv(test_data, names=['Job Title','Label'])\n",
    "    \n",
    "    print('df shape {}'.format(df.shape))\n",
    "    print('df_test shape {}'.format(df_test.shape))\n",
    "    \n",
    "\n",
    "\n",
    "    def clean_column(data):\n",
    "        if data is not None:\n",
    "            data =  data.lower()\n",
    "            data = re.sub('&nbsp', '', data)\n",
    "            data = re.sub(';', '', data)\n",
    "            data = re.sub('_', '', data)\n",
    "            data =re.sub(\"\\[[^]]*\\]\", \" \", data)\n",
    "            data = re.sub('<[^<]+?>', '', data)\n",
    "            data = re.sub(r'\\n',' ',data)\n",
    "            data = re.sub(r'[0-9]+','',data)\n",
    "            data = re.sub(r'@type','',data)\n",
    "            data = re.findall('[\\w]+',data)\n",
    "            return data\n",
    "        return 'Sorry'\n",
    "\n",
    "    df['Job Title'] = df['Job Title'].apply(clean_column)\n",
    "    df['Job Title'] = df['Job Title'].apply(str)\n",
    "    \n",
    "    #Processing test data\n",
    "    df_test['Job Title'] = df_test['Job Title'].apply(clean_column)\n",
    "    df_test['Job Title'] = df_test['Job Title'].apply(str)\n",
    "    \n",
    "    print('Data after cleaning : {}, number of categories: {}'.format(df.shape, df.Label.unique()))\n",
    "    split_ratio = 0.2\n",
    "    print('Splitting data into train and test sets with ratio {}'.format(split_ratio))\n",
    "    \n",
    " \n",
    "    X_train = df['Job Title']\n",
    "    X_test = df_test['Job Title']\n",
    "    print('X shape {}'.format(X_train.shape))\n",
    "\n",
    "    y_train = df['Label']\n",
    "    y_test = df_test['Label']\n",
    "    print('y_train shape {}'.format(y_train.shape))\n",
    "\n",
    "\n",
    "    print('Train data shape after pre-processing: {}'.format(X_train.shape))\n",
    "\n",
    "\n",
    "    train_features_output_path = os.path.join('/opt/ml/processing/output/train', 'train_features.csv')\n",
    "    train_labels_output_path = os.path.join('/opt/ml/processing/output/train', 'train_labels.csv')\n",
    "\n",
    "    test_features_output_path = os.path.join('/opt/ml/processing/output/test', 'test_features.csv')\n",
    "    test_labels_output_path = os.path.join('/opt/ml/processing/output/test', 'test_labels.csv')\n",
    "\n",
    "    print('Saving training features to {}'.format(train_features_output_path))\n",
    "    pd.DataFrame(X_train).to_csv(train_features_output_path, header=False, index=False)\n",
    "\n",
    "    print('Saving test features to {}'.format(test_features_output_path))\n",
    "    pd.DataFrame(X_test).to_csv(test_features_output_path, header=False, index=False)\n",
    "\n",
    "    print('Saving training labels to {}'.format(train_labels_output_path))\n",
    "    y_train.to_csv(train_labels_output_path, header=False, index=False)\n",
    "\n",
    "    print('Saving test labels to {}'.format(test_labels_output_path))\n",
    "    y_test.to_csv(test_labels_output_path, header=False, index=False)\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The ScriptProcessor class lets you run a command inside this container, which you can use to run your own script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.processing import ScriptProcessor\n",
    "\n",
    "script_processor = ScriptProcessor(command=['python3'],\n",
    "                image_uri= '589274001472.dkr.ecr.us-east-2.amazonaws.com/altru-recommend-development:c5b83c7beadc6388a19cd328e1008f97741fd749',\n",
    "                role=role,\n",
    "                instance_count=1,\n",
    "                instance_type='ml.m5.xlarge')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter 'session' will be renamed to 'sagemaker_session' in SageMaker Python SDK v2.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Job Name:  altru-recommend-development-2020-11-09-16-00-52-746\n",
      "Inputs:  [{'InputName': 'input-1', 'S3Input': {'S3Uri': 's3://altru-comprehend-input/Data_Sagemaker/train.csv', 'LocalPath': '/opt/ml/processing/input', 'S3DataType': 'S3Prefix', 'S3InputMode': 'File', 'S3DataDistributionType': 'FullyReplicated', 'S3CompressionType': 'None'}}, {'InputName': 'code', 'S3Input': {'S3Uri': 's3://sagemaker-us-east-2-589274001472/altru-recommend-development-2020-11-09-16-00-52-746/input/code/preprocessing.py', 'LocalPath': '/opt/ml/processing/input/code', 'S3DataType': 'S3Prefix', 'S3InputMode': 'File', 'S3DataDistributionType': 'FullyReplicated', 'S3CompressionType': 'None'}}]\n",
      "Outputs:  [{'OutputName': 'train_data', 'S3Output': {'S3Uri': 's3://sagemaker-us-east-2-589274001472/altru-recommend-development-2020-11-09-16-00-52-746/output/train_data', 'LocalPath': '/opt/ml/processing/output/train', 'S3UploadMode': 'EndOfJob'}}, {'OutputName': 'test_data', 'S3Output': {'S3Uri': 's3://sagemaker-us-east-2-589274001472/altru-recommend-development-2020-11-09-16-00-52-746/output/test_data', 'LocalPath': '/opt/ml/processing/output/test', 'S3UploadMode': 'EndOfJob'}}]\n",
      ".......................\u001b[34m[nltk_data] Downloading package wordnet to /root/nltk_data...\u001b[0m\n",
      "\u001b[34m[nltk_data]   Unzipping corpora/wordnet.zip.\u001b[0m\n",
      "\u001b[34m[nltk_data] Downloading package punkt to /root/nltk_data...\u001b[0m\n",
      "\u001b[34m[nltk_data]   Unzipping tokenizers/punkt.zip.\u001b[0m\n",
      "\u001b[34mReading input data from /opt/ml/input/data/train.csv\u001b[0m\n",
      "\u001b[34mdf shape (4533, 2)\u001b[0m\n",
      "\u001b[34mdf_test shape (2080, 2)\u001b[0m\n",
      "\u001b[34mData after cleaning : (4533, 2), number of categories: ['Merchandiser' 'Operations' 'Therapist' 'Human Resources' 'Counselor'\n",
      " 'Sales' 'Marketing' 'Technician' 'Doctor' 'Customer Service' 'Technology'\n",
      " 'Nurse' 'Accounting' 'Finance']\u001b[0m\n",
      "\u001b[34mSplitting data into train and test sets with ratio 0.2\u001b[0m\n",
      "\u001b[34mX shape (4533,)\u001b[0m\n",
      "\u001b[34my_train shape (4533,)\u001b[0m\n",
      "\u001b[34mTrain data shape after pre-processing: (4533,)\u001b[0m\n",
      "\u001b[34mSaving training features to /opt/ml/processing/output/train/train_features.csv\u001b[0m\n",
      "\u001b[34mSaving test features to /opt/ml/processing/output/test/test_features.csv\u001b[0m\n",
      "\u001b[34mSaving training labels to /opt/ml/processing/output/train/train_labels.csv\u001b[0m\n",
      "\u001b[34mSaving test labels to /opt/ml/processing/output/test/test_labels.csv\u001b[0m\n",
      "\n",
      "{'ProcessingInputs': [{'InputName': 'input-1', 'S3Input': {'S3Uri': 's3://altru-comprehend-input/Data_Sagemaker/train.csv', 'LocalPath': '/opt/ml/processing/input', 'S3DataType': 'S3Prefix', 'S3InputMode': 'File', 'S3DataDistributionType': 'FullyReplicated', 'S3CompressionType': 'None'}}, {'InputName': 'code', 'S3Input': {'S3Uri': 's3://sagemaker-us-east-2-589274001472/altru-recommend-development-2020-11-09-16-00-52-746/input/code/preprocessing.py', 'LocalPath': '/opt/ml/processing/input/code', 'S3DataType': 'S3Prefix', 'S3InputMode': 'File', 'S3DataDistributionType': 'FullyReplicated', 'S3CompressionType': 'None'}}], 'ProcessingOutputConfig': {'Outputs': [{'OutputName': 'train_data', 'S3Output': {'S3Uri': 's3://sagemaker-us-east-2-589274001472/altru-recommend-development-2020-11-09-16-00-52-746/output/train_data', 'LocalPath': '/opt/ml/processing/output/train', 'S3UploadMode': 'EndOfJob'}}, {'OutputName': 'test_data', 'S3Output': {'S3Uri': 's3://sagemaker-us-east-2-589274001472/altru-recommend-development-2020-11-09-16-00-52-746/output/test_data', 'LocalPath': '/opt/ml/processing/output/test', 'S3UploadMode': 'EndOfJob'}}]}, 'ProcessingJobName': 'altru-recommend-development-2020-11-09-16-00-52-746', 'ProcessingResources': {'ClusterConfig': {'InstanceCount': 1, 'InstanceType': 'ml.m5.xlarge', 'VolumeSizeInGB': 30}}, 'StoppingCondition': {'MaxRuntimeInSeconds': 86400}, 'AppSpecification': {'ImageUri': '589274001472.dkr.ecr.us-east-2.amazonaws.com/altru-recommend-development:c5b83c7beadc6388a19cd328e1008f97741fd749', 'ContainerEntrypoint': ['python3', '/opt/ml/processing/input/code/preprocessing.py']}, 'RoleArn': 'arn:aws:iam::589274001472:role/altru-sagemaker-role', 'ProcessingJobArn': 'arn:aws:sagemaker:us-east-2:589274001472:processing-job/altru-recommend-development-2020-11-09-16-00-52-746', 'ProcessingJobStatus': 'Completed', 'ProcessingEndTime': datetime.datetime(2020, 11, 9, 16, 4, 43, 58000, tzinfo=tzlocal()), 'ProcessingStartTime': datetime.datetime(2020, 11, 9, 16, 4, 24, 719000, tzinfo=tzlocal()), 'LastModifiedTime': datetime.datetime(2020, 11, 9, 16, 4, 43, 61000, tzinfo=tzlocal()), 'CreationTime': datetime.datetime(2020, 11, 9, 16, 0, 53, 164000, tzinfo=tzlocal()), 'ResponseMetadata': {'RequestId': '05e3a789-4164-4991-b323-92d0c7af91f2', 'HTTPStatusCode': 200, 'HTTPHeaders': {'x-amzn-requestid': '05e3a789-4164-4991-b323-92d0c7af91f2', 'content-type': 'application/x-amz-json-1.1', 'content-length': '2005', 'date': 'Mon, 09 Nov 2020 16:05:03 GMT'}, 'RetryAttempts': 0}}\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.processing import ProcessingInput, ProcessingOutput\n",
    "\n",
    "script_processor.run(code='preprocessing.py',\n",
    "                      inputs=[ProcessingInput(\n",
    "                        source=input_data,\n",
    "                        destination='/opt/ml/processing/input')],\n",
    "                      outputs=[ProcessingOutput(output_name='train_data',\n",
    "                                                source='/opt/ml/processing/output/train'),\n",
    "                               ProcessingOutput(output_name='test_data',\n",
    "                                                source='/opt/ml/processing/output/test')]\n",
    "                     )\n",
    "script_processor_job_description = script_processor.jobs[-1].describe()\n",
    "print(script_processor_job_description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_config = script_processor_job_description['ProcessingOutputConfig']\n",
    "output_config\n",
    "for output in output_config['Outputs']:\n",
    "    if output['OutputName'] == 'train_data':\n",
    "        preprocessed_training_data = output['S3Output']['S3Uri']\n",
    "    if output['OutputName'] == 'test_data':\n",
    "        preprocessed_test_data = output['S3Output']['S3Uri']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "This is not the latest supported version. If you would like to use version 0.23-1, please add framework_version=0.23-1 to your constructor.\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.sklearn.estimator import SKLearn\n",
    "\n",
    "sklearn = SKLearn(\n",
    "    entry_point='train.py',\n",
    "    train_instance_type=\"ml.m5.large\",\n",
    "    role=role)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting train.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile train.py\n",
    "#trying Decision Tree with experimental transformer\n",
    "import os\n",
    "\n",
    "import numpy\n",
    "import pandas as pd\n",
    "from io import StringIO\n",
    "from sagemaker_containers.beta.framework import (\n",
    "    content_types, encoders, env, modules, transformer, worker)\n",
    "from sklearn.externals import joblib\n",
    "import json\n",
    "\n",
    "if __name__==\"__main__\":\n",
    "    training_data_directory = '/opt/ml/input/data/train' #check this\n",
    "    train_features_data = os.path.join(training_data_directory, 'train_features.csv')\n",
    "    train_labels_data = os.path.join(training_data_directory, 'train_labels.csv')\n",
    "    print('Reading input data')\n",
    "    X_train = pd.read_csv(train_features_data, index_col = False, names = [\"Job Title\"], header = 0)#header=None,\n",
    "    print('X_train shape', X_train.shape)\n",
    "    print('Done reading the training features')\n",
    "    y_train = pd.read_csv(train_labels_data, index_col = False, names = [\"Labels\"], header = 0)# header=None,\n",
    "    print('y_train after changing to str', y_train.shape)\n",
    "    from sklearn.feature_extraction.text import TfidfTransformer\n",
    "    from sklearn.pipeline import Pipeline\n",
    "    \n",
    "    from sklearn.feature_extraction.text import CountVectorizer\n",
    "    from sklearn.feature_extraction.text import TfidfTransformer\n",
    "    import pandas as pd\n",
    "    \n",
    "    \n",
    "    import re\n",
    "\n",
    "\n",
    "    from sklearn.pipeline import Pipeline\n",
    "    \n",
    "    from sklearn.feature_extraction.text import CountVectorizer\n",
    "    from sklearn.feature_extraction.text import TfidfTransformer\n",
    "    #from sklearn.tree import DecisionTreeClassifier\n",
    "    import pandas as pd\n",
    "    from sklearn.naive_bayes import MultinomialNB\n",
    "    \n",
    "    text_clf = Pipeline([\n",
    "            ('vect', CountVectorizer(lowercase = False)),\n",
    "        ('tfidf', TfidfTransformer()),\n",
    "        ('clf', MultinomialNB()),\n",
    "        ])\n",
    "\n",
    "    \n",
    "    model = text_clf.fit(X_train['Job Title'].astype(str),y_train)\n",
    "\n",
    "    model_output_directory = os.path.join('/opt/ml/model', \"model.joblib\")\n",
    "    print('Saving model to {}'.format(model_output_directory))\n",
    "    joblib.dump(model, model_output_directory)\n",
    "\n",
    "def model_fn(model_dir):\n",
    "    \"\"\"Deserialized and return fitted model\n",
    "\n",
    "    Note that this should have the same name as the serialized model in the main method\n",
    "    \"\"\"\n",
    "    clf = joblib.load(os.path.join(model_dir, \"model.joblib\"))\n",
    "    return clf\n",
    "\n",
    "def input_fn(input_data, content_type):\n",
    "    \"\"\"Parse input data payload\n",
    "\n",
    "    We currently only take csv input. Since we need to process both labelled\n",
    "    and unlabelled data we first determine whether the label column is present\n",
    "    by looking at how many columns were provided.\n",
    "    \"\"\"\n",
    "    if content_type == 'text/csv':\n",
    "        # Read the raw input data as CSV.\n",
    "        df = pd.read_csv(StringIO(input_data), \n",
    "                         header=None, names = [\"Job Title\"])\n",
    "\n",
    "        return df['Job Title'].astype(str)\n",
    "    else:\n",
    "        raise ValueError(\"{} not supported by script!\".format(content_type))\n",
    "\n",
    "\n",
    "def output_fn(prediction, accept):\n",
    "    \"\"\"Format prediction output\n",
    "\n",
    "    The default accept/content-type between containers for serial inference is JSON.\n",
    "    We also want to set the ContentType or mimetype as the same value as accept so the next\n",
    "    container can read the response payload correctly.\n",
    "    \"\"\"\n",
    "    if accept == \"application/json\":\n",
    "        json_output = {\"instances\": prediction.tolist()}\n",
    "        return worker.Response(json.dumps(json_output), mimetype=accept)\n",
    "    elif accept == 'text/csv':\n",
    "        return worker.Response(encoders.encode(prediction, accept), mimetype=accept)\n",
    "    else:\n",
    "        raise RuntimeException(\"{} accept type is not supported by this script.\".format(accept))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting train.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile train.py\n",
    "#trying Decision Tree with experimental transformer\n",
    "import os\n",
    "\n",
    "import numpy\n",
    "import pandas as pd\n",
    "from io import StringIO\n",
    "from sagemaker_containers.beta.framework import (\n",
    "    content_types, encoders, env, modules, transformer, worker)\n",
    "from sklearn.externals import joblib\n",
    "import json\n",
    "\n",
    "if __name__==\"__main__\":\n",
    "    training_data_directory = '/opt/ml/input/data/train' #check this\n",
    "    train_features_data = os.path.join(training_data_directory, 'train_features.csv')\n",
    "    train_labels_data = os.path.join(training_data_directory, 'train_labels.csv')\n",
    "    print('Reading input data')\n",
    "    X_train = pd.read_csv(train_features_data, index_col = False, names = [\"Job Title\"], header = 0)#header=None,\n",
    "    print('X_train shape', X_train.shape)\n",
    "    print('Done reading the training features')\n",
    "    y_train = pd.read_csv(train_labels_data, index_col = False, names = [\"Labels\"], header = 0)# header=None,\n",
    "    print('y_train after changing to str', y_train.shape)\n",
    "    from sklearn.feature_extraction.text import TfidfTransformer\n",
    "    from sklearn.pipeline import Pipeline\n",
    "    \n",
    "    from sklearn.feature_extraction.text import CountVectorizer\n",
    "    from sklearn.feature_extraction.text import TfidfTransformer\n",
    "    from sklearn.tree import DecisionTreeClassifier\n",
    "    import pandas as pd\n",
    "    \n",
    "    \n",
    "    import re\n",
    "\n",
    "\n",
    "    from sklearn.pipeline import Pipeline\n",
    "    \n",
    "    from sklearn.feature_extraction.text import CountVectorizer\n",
    "    from sklearn.feature_extraction.text import TfidfTransformer\n",
    "    from sklearn.tree import DecisionTreeClassifier\n",
    "    import pandas as pd\n",
    "    #from sklearn.naive_bayes import MultinomialNB\n",
    "    \n",
    "    text_clf = Pipeline([\n",
    "            ('vect', CountVectorizer(lowercase = False)),\n",
    "        ('tfidf', TfidfTransformer()),\n",
    "        ('clf', DecisionTreeClassifier()),\n",
    "        ])\n",
    "\n",
    "    \n",
    "    model = text_clf.fit(X_train['Job Title'].astype(str),y_train)\n",
    "\n",
    "    model_output_directory = os.path.join('/opt/ml/model', \"model.joblib\")\n",
    "    print('Saving model to {}'.format(model_output_directory))\n",
    "    joblib.dump(model, model_output_directory)\n",
    "\n",
    "def model_fn(model_dir):\n",
    "    \"\"\"Deserialized and return fitted model\n",
    "\n",
    "    Note that this should have the same name as the serialized model in the main method\n",
    "    \"\"\"\n",
    "    clf = joblib.load(os.path.join(model_dir, \"model.joblib\"))\n",
    "    return clf\n",
    "\n",
    "def input_fn(input_data, content_type):\n",
    "    \"\"\"Parse input data payload\n",
    "\n",
    "    We currently only take csv input. Since we need to process both labelled\n",
    "    and unlabelled data we first determine whether the label column is present\n",
    "    by looking at how many columns were provided.\n",
    "    \"\"\"\n",
    "    if content_type == 'text/csv':\n",
    "        # Read the raw input data as CSV.\n",
    "        df = pd.read_csv(StringIO(input_data), \n",
    "                         header=None, names = [\"Job Title\"])\n",
    "\n",
    "        return df['Job Title'].astype(str)\n",
    "    else:\n",
    "        raise ValueError(\"{} not supported by script!\".format(content_type))\n",
    "\n",
    "\n",
    "def output_fn(prediction, accept):\n",
    "    \"\"\"Format prediction output\n",
    "\n",
    "    The default accept/content-type between containers for serial inference is JSON.\n",
    "    We also want to set the ContentType or mimetype as the same value as accept so the next\n",
    "    container can read the response payload correctly.\n",
    "    \"\"\"\n",
    "    if accept == \"application/json\":\n",
    "        json_output = {\"instances\": prediction.tolist()}\n",
    "        return worker.Response(json.dumps(json_output), mimetype=accept)\n",
    "    elif accept == 'text/csv':\n",
    "        return worker.Response(encoders.encode(prediction, accept), mimetype=accept)\n",
    "    else:\n",
    "        raise RuntimeException(\"{} accept type is not supported by this script.\".format(accept))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'s3_input' class will be renamed to 'TrainingInput' in SageMaker Python SDK v2.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-11-09 17:24:23 Starting - Starting the training job...\n",
      "2020-11-09 17:24:25 Starting - Launching requested ML instances......\n",
      "2020-11-09 17:25:30 Starting - Preparing the instances for training...\n",
      "2020-11-09 17:26:09 Downloading - Downloading input data..."
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "sklearn.fit({'train': preprocessed_training_data})\n",
    "training_job_description = sklearn.jobs[-1].describe()\n",
    "model_data_s3_uri = '{}{}/{}'.format(\n",
    "    training_job_description['OutputDataConfig']['S3OutputPath'],\n",
    "    training_job_description['TrainingJobName'],\n",
    "    'output/model.tar.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_data_s3_uri = '{}{}/{}'.format(\n",
    "    training_job_description['OutputDataConfig']['S3OutputPath'],\n",
    "    training_job_description['TrainingJobName'],\n",
    "    'output/model.tar.gz')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'s3://sagemaker-us-east-2-589274001472/sagemaker-scikit-learn-2020-11-07-16-13-21-597/output/model.tar.gz'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_data_s3_uri"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating the model with scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting evaluation.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile evaluation.py\n",
    "\n",
    "import json\n",
    "import os\n",
    "import tarfile\n",
    "import pandas as pd\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.metrics import classification_report, roc_auc_score, accuracy_score\n",
    "\n",
    "if __name__==\"__main__\":\n",
    "    model_path = os.path.join('/opt/ml/processing/model', 'model.tar.gz')\n",
    "    print('Extracting model from path: {}'.format(model_path))\n",
    "    with tarfile.open(model_path) as tar:\n",
    "        tar.extractall(path='.')\n",
    "    print('Loading model')\n",
    "    model = joblib.load('model.joblib')\n",
    "        \n",
    "\n",
    "    \n",
    "    #vect_path = os.path.join('/opt/ml/processing/model/vect', 'vectorizer.pkl')\n",
    "    #vectorizer = joblib.load('vectorizer.pkl')\n",
    "    \n",
    "    #Reading the test data\n",
    "    test_data_directory = 'opt/ml/processing/output/test'#'/opt/ml/input/data/test'\n",
    "    print('Loading test input data')\n",
    "    test_features_data = os.path.join(test_data_directory, 'test_features.csv')\n",
    "    test_labels_data = os.path.join(test_data_directory, 'test_labels.csv')\n",
    "\n",
    "    X_test = pd.read_csv(test_features_data, index_col = False, names = [\"Job Title\"], header = 0)\n",
    "    y_test = pd.read_csv(test_labels_data, index_col = False, names = [\"Labels\"], header = 0)\n",
    "    #X_test['Job Title'] = str(X_test['Job Title'])\n",
    "    \n",
    "    from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "    #from sklearn import metrics\n",
    "    #from sklearn.feature_extraction.text import CountVectorizer\n",
    "    #cv = CountVectorizer(lowercase = False, decode_error = 'ignore', encoding = 'string')\n",
    "    #vectorizer = TfidfVectorizer(lowercase = False)\n",
    "    #vectorised_X_test = vectorizer.transform(X_test['Job Title'].astype(str))\n",
    "    print('Vectorised X_test shape', X_test.shape)\n",
    "    print('y_test shape', y_test.shape)\n",
    "    \n",
    "     \n",
    "    predictions = model.predict(X_test['Job Title'].astype(str))\n",
    "\n",
    "    print('Creating classification evaluation report')\n",
    "    report_dict = classification_report(y_test, predictions, output_dict=True)\n",
    "    report_dict['accuracy'] = accuracy_score(y_test, predictions)\n",
    "    #report_dict['roc_auc'] = roc_auc_score(y_test, predictions)\n",
    "\n",
    "    print('Classification report:\\n{}'.format(report_dict))\n",
    "\n",
    "    evaluation_output_path = os.path.join('/opt/ml/processing/evaluation', 'evaluation.json')\n",
    "    print('Saving classification report to {}'.format(evaluation_output_path))\n",
    "\n",
    "    with open(evaluation_output_path, 'w') as f:\n",
    "        f.write(json.dumps(report_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter 'session' will be renamed to 'sagemaker_session' in SageMaker Python SDK v2.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Job Name:  sagemaker-scikit-learn-2020-11-07-16-18-34-866\n",
      "Inputs:  [{'InputName': 'input-1', 'S3Input': {'S3Uri': 's3://sagemaker-us-east-2-589274001472/sagemaker-scikit-learn-2020-11-07-16-13-21-597/output/model.tar.gz', 'LocalPath': '/opt/ml/processing/model', 'S3DataType': 'S3Prefix', 'S3InputMode': 'File', 'S3DataDistributionType': 'FullyReplicated', 'S3CompressionType': 'None'}}, {'InputName': 'input-2', 'S3Input': {'S3Uri': 's3://sagemaker-us-east-2-589274001472/altru-recommend-development-2020-11-07-16-07-51-058/output/test_data', 'LocalPath': '/opt/ml/processing/output/test', 'S3DataType': 'S3Prefix', 'S3InputMode': 'File', 'S3DataDistributionType': 'FullyReplicated', 'S3CompressionType': 'None'}}, {'InputName': 'code', 'S3Input': {'S3Uri': 's3://sagemaker-us-east-2-589274001472/sagemaker-scikit-learn-2020-11-07-16-18-34-866/input/code/evaluation.py', 'LocalPath': '/opt/ml/processing/input/code', 'S3DataType': 'S3Prefix', 'S3InputMode': 'File', 'S3DataDistributionType': 'FullyReplicated', 'S3CompressionType': 'None'}}]\n",
      "Outputs:  [{'OutputName': 'evaluation', 'S3Output': {'S3Uri': 's3://sagemaker-us-east-2-589274001472/sagemaker-scikit-learn-2020-11-07-16-18-34-866/output/evaluation', 'LocalPath': '/opt/ml/processing/evaluation', 'S3UploadMode': 'EndOfJob'}}]\n",
      ".....................\n",
      "\u001b[34m/miniconda3/lib/python3.7/site-packages/sklearn/externals/joblib/externals/cloudpickle/cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses\n",
      "  import imp\u001b[0m\n",
      "\u001b[34mExtracting model from path: /opt/ml/processing/model/model.tar.gz\u001b[0m\n",
      "\u001b[34mLoading model\u001b[0m\n",
      "\u001b[34mLoading test input data\u001b[0m\n",
      "\u001b[34mVectorised X_test shape (2079, 1)\u001b[0m\n",
      "\u001b[34my_test shape (2079, 1)\u001b[0m\n",
      "\u001b[34mCreating classification evaluation report\u001b[0m\n",
      "\u001b[34m/miniconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\u001b[0m\n",
      "\u001b[34mClassification report:\u001b[0m\n",
      "\u001b[34m{'Accounting': {'precision': 0.8790322580645161, 'recall': 0.8074074074074075, 'f1-score': 0.8416988416988417, 'support': 135}, 'Counselor': {'precision': 0.4392857142857143, 'recall': 0.924812030075188, 'f1-score': 0.5956416464891041, 'support': 133}, 'Customer Service': {'precision': 0.8198757763975155, 'recall': 0.8859060402684564, 'f1-score': 0.8516129032258064, 'support': 149}, 'Doctor': {'precision': 0.9851851851851852, 'recall': 0.9047619047619048, 'f1-score': 0.9432624113475178, 'support': 147}, 'Finance': {'precision': 0.7745664739884393, 'recall': 0.7976190476190477, 'f1-score': 0.7859237536656892, 'support': 168}, 'Human Resources': {'precision': 0.9652777777777778, 'recall': 0.8853503184713376, 'f1-score': 0.9235880398671097, 'support': 157}, 'Marketing': {'precision': 0.9411764705882353, 'recall': 0.9230769230769231, 'f1-score': 0.9320388349514563, 'support': 156}, 'Merchandiser': {'precision': 0.9539473684210527, 'recall': 0.9415584415584416, 'f1-score': 0.9477124183006537, 'support': 154}, 'Nurse': {'precision': 0.9209039548022598, 'recall': 0.9760479041916168, 'f1-score': 0.9476744186046512, 'support': 167}, 'Operations': {'precision': 0.6666666666666666, 'recall': 0.6029411764705882, 'f1-score': 0.6332046332046332, 'support': 136}, 'Sales': {'precision': 0.810126582278481, 'recall': 0.9078014184397163, 'f1-score': 0.8561872909698997, 'support': 141}, 'Technician': {'precision': 0.9307692307692308, 'recall': 0.9236641221374046, 'f1-score': 0.9272030651340997, 'support': 131}, 'Technology': {'precision': 0.757396449704142, 'recall': 0.8311688311688312, 'f1-score': 0.7925696594427244, 'support': 154}, 'Therapist': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 151}, 'micro avg': {'precision': 0.8085618085618086, 'recall': 0.8085618085618086, 'f1-score': 0.8085618085618085, 'support': 2079}, 'macro avg': {'precision': 0.7745864220663726, 'recall': 0.8080082546890617, 'f1-score': 0.7841655654930133, 'support': 2079}, 'weighted avg': {'precision': 0.7776529910240763, 'recall': 0.8085618085618086, 'f1-score': 0.7866296576927051, 'support': 2079}, 'accuracy': 0.8085618085618086}\u001b[0m\n",
      "\u001b[34mSaving classification report to /opt/ml/processing/evaluation/evaluation.json\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from sagemaker.s3 import S3Downloader\n",
    "\n",
    "sklearn_processor.run(code='evaluation.py',\n",
    "                      inputs=[ProcessingInput(\n",
    "                                  source=model_data_s3_uri,\n",
    "                                  destination='/opt/ml/processing/model'),\n",
    "                              ProcessingInput(\n",
    "                                  source=preprocessed_test_data,\n",
    "                                  destination='/opt/ml/processing/output/test')],\n",
    "                      outputs=[ProcessingOutput(output_name='evaluation',\n",
    "                                  source='/opt/ml/processing/evaluation')]\n",
    "                     )                    \n",
    "evaluation_job_description = sklearn_processor.jobs[-1].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"Accounting\": {\n",
      "        \"f1-score\": 0.8416988416988417,\n",
      "        \"precision\": 0.8790322580645161,\n",
      "        \"recall\": 0.8074074074074075,\n",
      "        \"support\": 135\n",
      "    },\n",
      "    \"Counselor\": {\n",
      "        \"f1-score\": 0.5956416464891041,\n",
      "        \"precision\": 0.4392857142857143,\n",
      "        \"recall\": 0.924812030075188,\n",
      "        \"support\": 133\n",
      "    },\n",
      "    \"Customer Service\": {\n",
      "        \"f1-score\": 0.8516129032258064,\n",
      "        \"precision\": 0.8198757763975155,\n",
      "        \"recall\": 0.8859060402684564,\n",
      "        \"support\": 149\n",
      "    },\n",
      "    \"Doctor\": {\n",
      "        \"f1-score\": 0.9432624113475178,\n",
      "        \"precision\": 0.9851851851851852,\n",
      "        \"recall\": 0.9047619047619048,\n",
      "        \"support\": 147\n",
      "    },\n",
      "    \"Finance\": {\n",
      "        \"f1-score\": 0.7859237536656892,\n",
      "        \"precision\": 0.7745664739884393,\n",
      "        \"recall\": 0.7976190476190477,\n",
      "        \"support\": 168\n",
      "    },\n",
      "    \"Human Resources\": {\n",
      "        \"f1-score\": 0.9235880398671097,\n",
      "        \"precision\": 0.9652777777777778,\n",
      "        \"recall\": 0.8853503184713376,\n",
      "        \"support\": 157\n",
      "    },\n",
      "    \"Marketing\": {\n",
      "        \"f1-score\": 0.9320388349514563,\n",
      "        \"precision\": 0.9411764705882353,\n",
      "        \"recall\": 0.9230769230769231,\n",
      "        \"support\": 156\n",
      "    },\n",
      "    \"Merchandiser\": {\n",
      "        \"f1-score\": 0.9477124183006537,\n",
      "        \"precision\": 0.9539473684210527,\n",
      "        \"recall\": 0.9415584415584416,\n",
      "        \"support\": 154\n",
      "    },\n",
      "    \"Nurse\": {\n",
      "        \"f1-score\": 0.9476744186046512,\n",
      "        \"precision\": 0.9209039548022598,\n",
      "        \"recall\": 0.9760479041916168,\n",
      "        \"support\": 167\n",
      "    },\n",
      "    \"Operations\": {\n",
      "        \"f1-score\": 0.6332046332046332,\n",
      "        \"precision\": 0.6666666666666666,\n",
      "        \"recall\": 0.6029411764705882,\n",
      "        \"support\": 136\n",
      "    },\n",
      "    \"Sales\": {\n",
      "        \"f1-score\": 0.8561872909698997,\n",
      "        \"precision\": 0.810126582278481,\n",
      "        \"recall\": 0.9078014184397163,\n",
      "        \"support\": 141\n",
      "    },\n",
      "    \"Technician\": {\n",
      "        \"f1-score\": 0.9272030651340997,\n",
      "        \"precision\": 0.9307692307692308,\n",
      "        \"recall\": 0.9236641221374046,\n",
      "        \"support\": 131\n",
      "    },\n",
      "    \"Technology\": {\n",
      "        \"f1-score\": 0.7925696594427244,\n",
      "        \"precision\": 0.757396449704142,\n",
      "        \"recall\": 0.8311688311688312,\n",
      "        \"support\": 154\n",
      "    },\n",
      "    \"Therapist\": {\n",
      "        \"f1-score\": 0.0,\n",
      "        \"precision\": 0.0,\n",
      "        \"recall\": 0.0,\n",
      "        \"support\": 151\n",
      "    },\n",
      "    \"accuracy\": 0.8085618085618086,\n",
      "    \"macro avg\": {\n",
      "        \"f1-score\": 0.7841655654930133,\n",
      "        \"precision\": 0.7745864220663726,\n",
      "        \"recall\": 0.8080082546890617,\n",
      "        \"support\": 2079\n",
      "    },\n",
      "    \"micro avg\": {\n",
      "        \"f1-score\": 0.8085618085618085,\n",
      "        \"precision\": 0.8085618085618086,\n",
      "        \"recall\": 0.8085618085618086,\n",
      "        \"support\": 2079\n",
      "    },\n",
      "    \"weighted avg\": {\n",
      "        \"f1-score\": 0.7866296576927051,\n",
      "        \"precision\": 0.7776529910240763,\n",
      "        \"recall\": 0.8085618085618086,\n",
      "        \"support\": 2079\n",
      "    }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "evaluation_output_config = evaluation_job_description['ProcessingOutputConfig']\n",
    "for output in evaluation_output_config['Outputs']:\n",
    "    if output['OutputName'] == 'evaluation':\n",
    "        evaluation_s3_uri = output['S3Output']['S3Uri'] + '/evaluation.json'\n",
    "        break\n",
    "\n",
    "evaluation_output = S3Downloader.read_file(evaluation_s3_uri)\n",
    "evaluation_output_dict = json.loads(evaluation_output)\n",
    "print(json.dumps(evaluation_output_dict, sort_keys=True, indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating EndPoint for real time prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter image will be renamed to image_uri in SageMaker Python SDK v2.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------!"
     ]
    }
   ],
   "source": [
    "# calling predictor function\n",
    "endpoint_name = \"Altru-JobTitleClassifier-Endpoint\"\n",
    "predictor = sklearn.deploy(initial_instance_count=1, instance_type=\"ml.t2.medium\", endpoint_name = endpoint_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.content_types import CONTENT_TYPE_CSV, CONTENT_TYPE_JSON\n",
    "\n",
    "predictor = sagemaker.predictor.RealTimePredictor(endpoint=endpoint_name,   #create predictor to send serialized data to sagemaker\n",
    "                                                serializer=sagemaker.predictor.csv_serializer,\n",
    "                                                content_type='text/csv', accept=CONTENT_TYPE_JSON)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'{\"instances\": [\"Sales\"]}'\n"
     ]
    }
   ],
   "source": [
    "payload = 'sales consultant'\n",
    "print(predictor.predict(payload))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    VP, Americas NFA/CFTC ex-trading, Cash Solicit...\n",
       "2    508 Compliance Expert\\n\\nPerforms 508 complian...\n",
       "3    Compliance Officer\\n\\nEvaluate business activi...\n",
       "4    Engineering Intern- Summer 2021\\n\\nEngineering...\n",
       "Name: Job Role, dtype: object"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "test_data = pd.read_csv('validate_BT.csv', names = ['Job Role', 'Label'])\n",
    "test_data.head()\n",
    "test_X = test_data.iloc[1:5,0]\n",
    "test_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "predictor.delete_endpoint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
